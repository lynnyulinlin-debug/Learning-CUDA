#include <vector>
#include <common/maca_fp16.h>
#include <hip/hip_runtime.h>
#include <hip/hip_fp16.h>

#include <type_traits>
#include <cmath>
#include <limits>
#include <algorithm>
#include <cstdlib>
#include <iostream>

#include "../tester/utils.h"

// ============================================================================
// MACA (MetaX HIP-compatible) Runtime Check Macro
// ============================================================================
#define RUNTIME_CHECK(call)                                                   \
  do {                                                                        \
    hipError_t err = (call);                                                  \
    if (err != hipSuccess) {                                                  \
      std::cerr << "MACA Runtime error: " << hipGetErrorString(err)           \
                << " at " << __FILE__ << ":" << __LINE__ << "\n";             \
      exit(EXIT_FAILURE);                                                     \
    }                                                                         \
  } while (0)

// ============================================================================
// Runtime mapping (MACA / MetaX - HIP compatible)
// ============================================================================
#if defined(PLATFORM_METAX)
  #define DEV_MALLOC        hipMalloc
  #define DEV_FREE          hipFree
  #define DEV_MEMCPY        hipMemcpy
  #define DEV_MEMSET        hipMemset
  #define DEV_DEVICE_SYNC   hipDeviceSynchronize
  #define DEV_GET_LAST_ERR  hipGetLastError
  #define DEV_ERR_STR       hipGetErrorString

  #define MEMCPY_H2D        hipMemcpyHostToDevice
  #define MEMCPY_D2H        hipMemcpyDeviceToHost
  #define MEMCPY_D2D        hipMemcpyDeviceToDevice
#else
  #error "This kernels.maca is only for MetaX platform."
#endif

// kernel launch check
#define KERNEL_LAUNCH_CHECK()                                         \
  do {                                                                \
    auto e = DEV_GET_LAST_ERR();                                      \
    if (e != hipSuccess) {                                            \
      std::cerr << "Kernel launch error: " << DEV_ERR_STR(e) << "\n"; \
      exit(EXIT_FAILURE);                                             \
    }                                                                 \
    RUNTIME_CHECK(DEV_DEVICE_SYNC());                                 \
  } while (0)

// ============================================================================
// Device helpers
// ============================================================================
template <typename T>
__device__ __forceinline__ float to_float_dev(T x) {
  if constexpr (std::is_same_v<T, __half>) return __half2float(x);
  else return (float)x;
}

template <typename T>
__device__ __forceinline__ T from_float_dev(float x) {
  if constexpr (std::is_same_v<T, __half>) return __float2half(x);
  else return (T)x;
}


/**
 * @brief Computes the trace of a matrix.
 *
 * The trace of a matrix is defined as the sum of its diagonal elements.
 * This function expects a flattened row-major matrix stored in a
 * std::vector. If the matrix is not square, the trace will sum up
 * elements along the main diagonal up to the smaller of rows or cols.
 *
 * @tparam T The numeric type of matrix elements (e.g., float, int).
 * @param h_input A flattened matrix of size rows * cols.
 * @param rows Number of rows in the matrix.
 * @param cols Number of columns in the matrix.
 * @return The trace (sum of diagonal values) of the matrix.
 */
// ============================================================================
// 1） trace kernel
// ============================================================================
template <typename T>
__global__ void trace_kernel(const T* __restrict__ in, size_t rows, size_t cols, T* __restrict__ out) {
  const size_t n = (rows < cols) ? rows : cols;
  size_t tid = (size_t)blockIdx.x * blockDim.x + threadIdx.x;
  T local = (T)0;
  const size_t stride = (size_t)gridDim.x * blockDim.x;
  for (size_t i = tid; i < n; i += stride) {
    local = local + in[i * cols + i];
  }

  // Use HIP dynamic shared memory
  extern __shared__ unsigned char smem_raw[];
  T* smem = reinterpret_cast<T*>(smem_raw);
  smem[threadIdx.x] = local;
  __syncthreads();

  for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
    if (threadIdx.x < s) smem[threadIdx.x] = smem[threadIdx.x] + smem[threadIdx.x + s];
    __syncthreads();
  }

  if (threadIdx.x == 0) {
    atomicAdd(out, smem[0]);
  }
}


template <typename T>
T trace(const std::vector<T>& h_input, size_t rows, size_t cols) {
  // TODO: Implement the trace function
  //return T(-1);
  
  const size_t n_elem = rows * cols;
  if (n_elem == 0) return (T)0;

  T *d_in = nullptr, *d_out = nullptr;
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_in, n_elem * sizeof(T)));
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_out, sizeof(T)));

  RUNTIME_CHECK(DEV_MEMCPY(d_in, h_input.data(), n_elem * sizeof(T), MEMCPY_H2D));
  RUNTIME_CHECK(DEV_MEMSET(d_out, 0, sizeof(T)));

  const int threads = 256;
  const size_t n_diag = (rows < cols) ? rows : cols;
  int blocks = (int)((n_diag + (size_t)threads - 1) / (size_t)threads);
  blocks = std::max(std::min(blocks, 120), 1);

  const size_t shmem = (size_t)threads * sizeof(T);
  hipLaunchKernelGGL(trace_kernel<T>, dim3(blocks), dim3(threads), shmem, 0, 
                     d_in, rows, cols, d_out);
  KERNEL_LAUNCH_CHECK();

  T h_out;
  RUNTIME_CHECK(DEV_MEMCPY(&h_out, d_out, sizeof(T), MEMCPY_D2H));

  RUNTIME_CHECK(DEV_FREE(d_in));
  RUNTIME_CHECK(DEV_FREE(d_out));
  return h_out;

  
}

/**
 * @brief Computes flash attention for given query, key, and value tensors.
 * 
 * @tparam T Data type (float) for input/output tensors
 * @param[in] h_q Query tensor of shape [batch_size, tgt_seq_len, query_heads, head_dim]
 * @param[in] h_k Key tensor of shape [batch_size, src_seq_len, kv_heads, head_dim]
 * @param[in] h_v Value tensor of shape [batch_size, src_seq_len, kv_heads, head_dim]
 * @param[out] h_o Output attention tensor of shape [batch_size, tgt_seq_len, query_heads, head_dim]
 * @param[in] batch_size Batch dimension size
 * @param[in] target_seq_len Target sequence length
 * @param[in] src_seq_len Source sequence length  
 * @param[in] query_heads Number of query attention heads
 * @param[in] kv_heads Number of key/value heads (supports grouped query attention)
 * @param[in] head_dim Dimension size of each attention head
 * @param[in] is_causal Whether to apply causal masking
 */

// ============================================================================
// 2） flashAttention kernel
// ============================================================================
template <typename T>
__global__ void flashattn_kernel(const T* __restrict__ q,
                                 const T* __restrict__ k,
                                 const T* __restrict__ v,
                                 T* __restrict__ o,
                                 int B, int Tt, int Ss, int QH, int KVH, int D, 
                                 bool causal) {
  const int b  = (int)blockIdx.x;
  const int t  = (int)blockIdx.y;
  const int qh = (int)blockIdx.z;
  const int d  = (int)threadIdx.x;

  if (b >= B || t >= Tt || qh >= QH || d >= D) return;

  int kvh = 0;
  if (KVH > 0 && QH == KVH) kvh = qh;
  else if (KVH > 0 && (QH % KVH == 0)) kvh = qh / (QH / KVH);
  else if (KVH > 0) kvh = qh % KVH;

  const float scale = 1.0f / sqrtf((float)D);

  const size_t q_base = ((size_t)b * (size_t)Tt * (size_t)QH * (size_t)D)
                      + ((size_t)t * (size_t)QH * (size_t)D)
                      + ((size_t)qh * (size_t)D);
  const size_t o_base = q_base;

  __shared__ float sh_max;
  __shared__ float sh_p;
  __shared__ float sh_sum;

  // Pass 1: compute max score
  if (d == 0) {
    float m = -INFINITY;
    for (int s = 0; s < Ss; ++s) {
      if (causal && s > t) continue;

      const size_t k_base = ((size_t)b * (size_t)Ss * (size_t)KVH * (size_t)D)
                          + ((size_t)s * (size_t)KVH * (size_t)D)
                          + ((size_t)kvh * (size_t)D);

      float dot = 0.0f;
      for (int dd = 0; dd < D; ++dd) {
        dot = fmaf(to_float_dev(q[q_base + (size_t)dd]),
                   to_float_dev(k[k_base + (size_t)dd]),
                   dot);
      }
      const float score = dot * scale;
      m = fmaxf(m, score);
    }
    sh_max = m;
  }
  __syncthreads();

  // Pass 2: compute sum and output
  const float m = sh_max;

  float out = 0.0f;
  float sum = 0.0f;

  for (int s = 0; s < Ss; ++s) {
    if (causal && s > t) continue;

    const size_t k_base = ((size_t)b * (size_t)Ss * (size_t)KVH * (size_t)D)
                        + ((size_t)s * (size_t)KVH * (size_t)D)
                        + ((size_t)kvh * (size_t)D);
    const size_t v_base = k_base;

    if (d == 0) {
      float dot = 0.0f;
      for (int dd = 0; dd < D; ++dd) {
        dot = fmaf(to_float_dev(q[q_base + (size_t)dd]),
                   to_float_dev(k[k_base + (size_t)dd]),
                   dot);
      }
      const float score = dot * scale;
      sh_p = expf(score - m);
    }
    __syncthreads();

    const float p = sh_p;
    sum += p;
    out += p * to_float_dev(v[v_base + (size_t)d]);

    __syncthreads();
  }

  if (d == 0) sh_sum = sum;
  __syncthreads();

  const float denom = sh_sum;
  if (denom > 0.0f) out /= denom;

  o[o_base + (size_t)d] = from_float_dev<T>(out);
}

template <typename T>
void flashAttention(const std::vector<T>& h_q, const std::vector<T>& h_k,
                    const std::vector<T>& h_v, std::vector<T>& h_o,
                    int batch_size, int target_seq_len, int src_seq_len, 
                    int query_heads, int kv_heads, int head_dim, bool is_causal) {   
  const int B = batch_size, Tt = target_seq_len, Ss = src_seq_len;
  const int QH = query_heads, KVH = kv_heads, D = head_dim;

  const size_t q_sz = (size_t)B * (size_t)Tt * (size_t)QH * (size_t)D;
  const size_t k_sz = (size_t)B * (size_t)Ss * (size_t)KVH * (size_t)D;
  const size_t v_sz = (size_t)B * (size_t)Ss * (size_t)KVH * (size_t)D;
  const size_t o_sz = (size_t)B * (size_t)Tt * (size_t)QH * (size_t)D;

  h_o.resize(o_sz);

  T *d_q=nullptr, *d_k=nullptr, *d_v=nullptr, *d_o=nullptr;
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_q, q_sz * sizeof(T)));
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_k, k_sz * sizeof(T)));
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_v, v_sz * sizeof(T)));
  RUNTIME_CHECK(DEV_MALLOC((void**)&d_o, o_sz * sizeof(T)));

  RUNTIME_CHECK(DEV_MEMCPY(d_q, h_q.data(), q_sz * sizeof(T), MEMCPY_H2D));
  RUNTIME_CHECK(DEV_MEMCPY(d_k, h_k.data(), k_sz * sizeof(T), MEMCPY_H2D));
  RUNTIME_CHECK(DEV_MEMCPY(d_v, h_v.data(), v_sz * sizeof(T), MEMCPY_H2D));

  dim3 grid((unsigned)B, (unsigned)Tt, (unsigned)QH);
  dim3 block((unsigned)D, 1, 1);

  hipLaunchKernelGGL(flashattn_kernel<T>, grid, block, 0, 0, 
                     d_q, d_k, d_v, d_o, B, Tt, Ss, QH, KVH, D, is_causal);
  RUNTIME_CHECK(DEV_DEVICE_SYNC());

  RUNTIME_CHECK(DEV_MEMCPY(h_o.data(), d_o, o_sz * sizeof(T), MEMCPY_D2H));

  RUNTIME_CHECK(DEV_FREE(d_q));
  RUNTIME_CHECK(DEV_FREE(d_k));
  RUNTIME_CHECK(DEV_FREE(d_v));
  RUNTIME_CHECK(DEV_FREE(d_o));    
}

// *********************************************************************
// Explicit Template Instantiations (REQUIRED FOR LINKING WITH TESTER.O)
// DO NOT MODIFY THIS SECTION
// *********************************************************************
template int trace<int>(const std::vector<int>&, size_t, size_t);
template float trace<float>(const std::vector<float>&, size_t, size_t);
template void flashAttention<float>(const std::vector<float>&, const std::vector<float>&,
  const std::vector<float>&, std::vector<float>&,
  int, int, int, int, int, int, bool);
template void flashAttention<half>(const std::vector<half>&, const std::vector<half>&,
  const std::vector<half>&, std::vector<half>&,
  int, int, int, int, int, int, bool);
